loading model!
INFO 05-15 18:26:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/home/liyinghao/models/llama-2-7b', tokenizer='/home/liyinghao/models/llama-2-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 05-15 18:26:09 selector.py:16] Using FlashAttention backend.
INFO 05-15 18:26:16 model_runner.py:104] Loading model weights took 12.5523 GB
INFO 05-15 18:26:21 gpu_executor.py:94] # GPU blocks: 2833, # CPU blocks: 512
INFO 05-15 18:26:22 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-15 18:26:22 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-15 18:26:27 model_runner.py:867] Graph capturing finished in 5 secs.
finish loading model
----------
<vllm.entrypoints.llm.LLM object at 0x152151173450>
----------
start loading dataset!
start applying template
Loading URIAL prompt from /home/liyinghao/codes/URIAL/urial_prompts/inst_1k_v4.txt
Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 1/1 [00:00<00:00, 30.15 examples/s]
loading dataset ... done!
We skipped the first 0 examples
Generating llama-2-7b from 0 to 1:   0%|          | 0/1 [00:00<?, ?it/s]Generating llama-2-7b from 0 to 1: 100%|██████████| 1/1 [00:11<00:00, 11.16s/it]Generating llama-2-7b from 0 to 1: 100%|██████████| 1/1 [00:11<00:00, 11.16s/it]
